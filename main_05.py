import streamlit as st
import tempfile
import os
from dotenv import load_dotenv
load_dotenv()

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma  
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'chromadb' not in st.session_state:
    st.session_state.chromadb = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_processed' not in st.session_state:
    st.session_state.pdf_processed = False

def load_and_process_pdf(uploaded_file):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        # PDF ë¡œë” ì„¤ì •
        loader = PyPDFLoader(tmp_file_path)
        pages = loader.load()
        
        # í…ìŠ¤íŠ¸ ë¶„ë¦¬
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=50,
            length_function=len,
            is_separator_regex=False
        )
        texts = text_splitter.split_documents(pages)
        
        # OpenAI Embeddings ëª¨ë¸ ë¡œë“œ
        embeddings_model = OpenAIEmbeddings(model="text-embedding-ada-002")
        
        # ChromaDB ìƒì„±
        chromadb = Chroma.from_documents(
            documents=texts,
            embedding=embeddings_model,
            collection_name='pdf_documents'
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_file_path)
        
        return chromadb, len(texts)
        
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None, 0

def create_lcel_chain(chromadb):
    """LCEL ì²´ì¸ ìƒì„±"""
    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)
    
    prompt = PromptTemplate.from_template(
        """ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        
ë¬¸ì„œ: {context}
ì§ˆë¬¸: {question}

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.
ë‹µë³€:"""
    )
    
    lcel_chain = (
        {"context": chromadb.as_retriever(search_kwargs={"k": 3}), 
         "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return lcel_chain

def get_answer(question, chromadb):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        lcel_chain = create_lcel_chain(chromadb)
        result = lcel_chain.invoke(question)
        return result
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ë©”ì¸ ì•±
def main():
    st.title("ğŸ“š PDF RAG ì±—ë´‡")
    st.markdown("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    
    # ì‚¬ì´ë“œë°” - PDF ì—…ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ“¤ PDF ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['pdf'],
            help="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤."
        )
        
        if uploaded_file is not None:
            if st.button("ğŸ“Š ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°"):
                with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    chromadb, chunk_count = load_and_process_pdf(uploaded_file)
                    
                    if chromadb:
                        st.session_state.chromadb = chromadb
                        st.session_state.pdf_processed = True
                        st.success(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!")
                        st.info(f"ğŸ“„ ì´ {chunk_count}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.session_state.pdf_processed = False
        
        # ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
        if st.session_state.pdf_processed:
            st.success("ğŸŸ¢ PDF ì²˜ë¦¬ ì™„ë£Œ")
        else:
            st.warning("ğŸŸ¡ PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
    
    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’¬ ì±—ë´‡")
        
        # PDFê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°
        if not st.session_state.pdf_processed:
            st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            st.markdown("""
            ### ì‚¬ìš© ë°©ë²•:
            1. ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œ
            2. "ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°" ë²„íŠ¼ í´ë¦­
            3. ì²˜ë¦¬ ì™„ë£Œ í›„ ì§ˆë¬¸ ì…ë ¥
            """)
        else:
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
            for i, (q, a) in enumerate(st.session_state.chat_history):
                with st.container():
                    st.markdown(f"**ğŸ™‹â€â™‚ï¸ ì§ˆë¬¸:** {q}")
                    st.markdown(f"**ğŸ¤– ë‹µë³€:** {a}")
                    st.divider()
            
            # ì§ˆë¬¸ ì…ë ¥
            question = st.text_input(
                "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="ì˜ˆ: ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                key="question_input"
            )
            
            col_btn1, col_btn2 = st.columns([1, 4])
            with col_btn1:
                ask_button = st.button("ğŸ” ì§ˆë¬¸í•˜ê¸°", type="primary")
            with col_btn2:
                if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì‚­ì œ"):
                    st.session_state.chat_history = []
                    st.rerun()
            
            # ì§ˆë¬¸ ì²˜ë¦¬
            if ask_button and question.strip():
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    answer = get_answer(question, st.session_state.chromadb)
                    st.session_state.chat_history.append((question, answer))
                    st.rerun()
            elif ask_button and not question.strip():
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    
    with col2:
        st.header("â„¹ï¸ ì •ë³´")
        
        if st.session_state.pdf_processed:
            st.success("ğŸ“„ PDF ë¶„ì„ ì™„ë£Œ")
            st.markdown("**âœ¨ ê¸°ëŠ¥:**")
            st.markdown("- ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰")
            st.markdown("- ì§ˆì˜ì‘ë‹µ")
            st.markdown("- ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰")
        else:
            st.info("PDF ì—…ë¡œë“œ ëŒ€ê¸°ì¤‘")
        
        st.markdown("---")
        st.markdown("**ğŸ”§ ì‚¬ìš©ëœ ê¸°ìˆ :**")
        st.markdown("- LangChain")
        st.markdown("- OpenAI GPT-3.5")
        st.markdown("- ChromaDB")
        st.markdown("- Streamlit")
        
        # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
        if st.session_state.pdf_processed:
            st.markdown("---")
            st.markdown("**ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:**")
            example_questions = [
                "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
                "ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "íŠ¹ì • ê°œë…ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ëŠ”?",
                "ê²°ë¡ ì´ë‚˜ ìš”ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
            
            for eq in example_questions:
                if st.button(f"ğŸ“ {eq}", key=f"example_{hash(eq)}", help="í´ë¦­í•˜ë©´ ì§ˆë¬¸ì´ ì…ë ¥ë©ë‹ˆë‹¤"):
                    st.session_state.question_input = eq

if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        st.error("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    main()